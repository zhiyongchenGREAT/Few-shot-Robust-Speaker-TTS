{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# control policy 1: synthetic unknown speakers (σ = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "libritts_path = \"./unknown_embedding_folder\"\n",
    "train3_path = \"./target_embedding_folder\"\n",
    "output_file = \"./cosine_sim_unknown_synspk.txt\"\n",
    "\n",
    "def load_embeddings_from_folder(folder_path):\n",
    "    embeddings = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".npy\"):\n",
    "            embedding = np.load(os.path.join(folder_path, file))\n",
    "            embeddings.append(embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1_norm = vec1 / np.linalg.norm(vec1)\n",
    "    vec2_norm = vec2 / np.linalg.norm(vec2)\n",
    "    return np.dot(vec1_norm, vec2_norm)\n",
    "\n",
    "target_ids = [tid for tid in os.listdir(train3_path) if os.path.isdir(os.path.join(train3_path, tid))]\n",
    "\n",
    "target_mean_embeddings = {}\n",
    "for target_id in target_ids:\n",
    "    target_folder = os.path.join(train3_path, target_id)\n",
    "    target_embeddings = load_embeddings_from_folder(target_folder)\n",
    "    if len(target_embeddings) > 0:\n",
    "        target_mean_embeddings[target_id] = np.mean(target_embeddings, axis=0)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    \n",
    "    f.write(\"Speaker_ID\\t\" + \"\\t\".join(target_ids) + \"\\n\")\n",
    "    \n",
    "    for speaker_id in os.listdir(libritts_path):\n",
    "        speaker_folder = os.path.join(libritts_path, speaker_id)\n",
    "        if not os.path.isdir(speaker_folder):\n",
    "            continue\n",
    "        \n",
    "        speaker_embeddings = load_embeddings_from_folder(speaker_folder)\n",
    "        if len(speaker_embeddings) == 0:\n",
    "            continue\n",
    "        speaker_embedding = speaker_embeddings[0]\n",
    "        \n",
    "        # cosine similarity\n",
    "        similarity_scores = []\n",
    "        for target_id in target_ids:\n",
    "            if target_id in target_mean_embeddings:\n",
    "                similarity_score = cosine_similarity(speaker_embedding, target_mean_embeddings[target_id])\n",
    "                similarity_scores.append(f\"{similarity_score:.4f}\")\n",
    "            else:\n",
    "                similarity_scores.append(\"N/A\")\n",
    "        \n",
    "        f.write(f\"{speaker_id}\\t\" + \"\\t\".join(similarity_scores) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = \"./cosine_sim_unknown_synspk.txt\"\n",
    "\n",
    "df = pd.read_csv(input_file, sep=\"\\t\")\n",
    "df['Average_Score'] = df.iloc[:, 1:].mean(axis=1) \n",
    "\n",
    "filtered_df = df[df['Average_Score'] < 0.3]\n",
    "\n",
    "selected_speakers = []\n",
    "target_scores = filtered_df.iloc[:, 1:-1]\n",
    "\n",
    "picked_speakers = set()\n",
    "\n",
    "while len(selected_speakers) < 12:\n",
    "    for target in target_scores.columns:\n",
    "        if len(selected_speakers) >= 12:\n",
    "            break\n",
    "        \n",
    "        target_sorted = filtered_df.sort_values(by=target)\n",
    "        \n",
    "        for _, row in target_sorted.iterrows():\n",
    "            if row['Speaker_ID'] not in picked_speakers:\n",
    "                selected_speakers.append({\n",
    "                    \"Speaker_ID\": int(row['Speaker_ID']), \n",
    "                    \"Target\": target,\n",
    "                    \"Score\": row[target]\n",
    "                })\n",
    "                picked_speakers.add(row['Speaker_ID'])\n",
    "                break\n",
    "        \n",
    "        for _, row in target_sorted.iloc[::-1].iterrows():\n",
    "            if row['Speaker_ID'] not in picked_speakers:\n",
    "                selected_speakers.append({\n",
    "                    \"Speaker_ID\": int(row['Speaker_ID']),  \n",
    "                    \"Target\": target,\n",
    "                    \"Score\": row[target]\n",
    "                })\n",
    "                picked_speakers.add(row['Speaker_ID'])\n",
    "                break\n",
    "\n",
    "result_df = pd.DataFrame(selected_speakers)\n",
    "\n",
    "speaker_list = result_df['Speaker_ID'].astype(str).tolist()\n",
    "formatted_speaker_list = \", \".join([f\"'{s}'\" for s in speaker_list])\n",
    "\n",
    "print(\"Selected Speakers:\")\n",
    "print(result_df)\n",
    "\n",
    "print(\"\\nFormatted Speaker List:\")\n",
    "print(f\"[{formatted_speaker_list}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# control policy 2: nearest-N target synthetic samples (N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "gptsovits_emb_dir = \"./target_spk_syn_emb\"\n",
    "target_emb_dir = \"./target_spk_enroll_emb\"\n",
    "output_file = \"./cosine_sim_targetsyn.txt\"\n",
    "\n",
    "def load_target_embeddings(target_emb_dir):\n",
    "    target_means = []\n",
    "    target_ids = []\n",
    "\n",
    "    for target_id in sorted(os.listdir(target_emb_dir)): \n",
    "        target_path = os.path.join(target_emb_dir, target_id)\n",
    "        if os.path.isdir(target_path):\n",
    "            embeddings = []\n",
    "            for file in sorted(os.listdir(target_path)): \n",
    "                if file.endswith(\".npy\"):\n",
    "                    emb = np.load(os.path.join(target_path, file))\n",
    "                    # emb = torch.load(os.path.join(target_path, file))\n",
    "                    embeddings.append(emb)\n",
    "            if embeddings:\n",
    "                target_means.append(np.mean(embeddings, axis=0)) \n",
    "                target_ids.append(target_id)\n",
    "\n",
    "    return target_ids, np.array(target_means)\n",
    "\n",
    "def load_speaker_embeddings(gptsovits_emb_dir):\n",
    "    speaker_embeddings = []  \n",
    "    for speaker_id in sorted(os.listdir(gptsovits_emb_dir)): \n",
    "        speaker_path = os.path.join(gptsovits_emb_dir, speaker_id)\n",
    "        if os.path.isdir(speaker_path):\n",
    "            for file in sorted(os.listdir(speaker_path)): \n",
    "                if file.endswith(\".npy\"):\n",
    "                    emb = np.load(os.path.join(speaker_path, file))\n",
    "                    speaker_embeddings.append((speaker_id, file.replace(\".npy\", \"\"), emb))\n",
    "    return speaker_embeddings\n",
    "\n",
    "def calculate_similarity(speaker_embeddings, target_ids, target_means):\n",
    "    results = [] \n",
    "\n",
    "    for speaker_id, emb_id, emb in speaker_embeddings:\n",
    "        similarities = cosine_similarity([emb], target_means)[0] \n",
    "        results.append((speaker_id, emb_id, similarities))\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_results(output_file, target_ids, results):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"Speaker_ID\\tEmbed_ID\\t\" + \"\\t\".join(target_ids) + \"\\n\")\n",
    "\n",
    "        for speaker_id, emb_id, similarities in results:\n",
    "            similarity_str = \"\\t\".join(f\"{sim:.4f}\" for sim in similarities)\n",
    "            f.write(f\"{speaker_id}\\t{emb_id}\\t{similarity_str}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading target embeddings...\")\n",
    "    target_ids, target_means = load_target_embeddings(target_emb_dir)\n",
    "\n",
    "    print(\"Loading speaker embeddings...\")\n",
    "    speaker_embeddings = load_speaker_embeddings(gptsovits_emb_dir)\n",
    "\n",
    "    print(\"Calculating cosine similarities...\")\n",
    "    results = calculate_similarity(speaker_embeddings, target_ids, target_means)\n",
    "\n",
    "    print(f\"Saving results to {output_file}...\")\n",
    "    save_results(output_file, target_ids, results)\n",
    "\n",
    "    print(\"✅ All cosine similarities have been calculated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = \"./cosine_sim_targetsyn.txt\"\n",
    "df = pd.read_csv(input_file, sep=\"\\t\")\n",
    "\n",
    "df[\"Speaker_ID\"] = df[\"Speaker_ID\"].astype(str)\n",
    "df.iloc[:, 2:] = df.iloc[:, 2:].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "def find_top_n_sentences(df, speaker_id, target_col, n=3):\n",
    "    speaker_data = df[df[\"Speaker_ID\"] == speaker_id]\n",
    "    if speaker_data.empty:\n",
    "        print(f\"No matching rows for Speaker_ID {speaker_id}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"Column {target_col} not found in dataframe!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    speaker_data[target_col] = pd.to_numeric(speaker_data[target_col], errors='coerce')\n",
    "    \n",
    "    top_n = speaker_data.nlargest(n, target_col)\n",
    "    return top_n[[\"Embed_ID\", target_col]]\n",
    "\n",
    "# target synthetic samples\n",
    "\n",
    "queries = [\n",
    "    {\"speaker_id\": \"0\", \"target_col\": \"0\", \"top_n\": 3},\n",
    "    {\"speaker_id\": \"1\", \"target_col\": \"1\", \"top_n\": 3},\n",
    "    {\"speaker_id\": \"2\", \"target_col\": \"2\", \"top_n\": 3},\n",
    "    {\"speaker_id\": \"3\", \"target_col\": \"3\", \"top_n\": 3},\n",
    "    {\"speaker_id\": \"4\", \"target_col\": \"4\", \"top_n\": 3},\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    speaker_id = query[\"speaker_id\"]\n",
    "    target_col = query[\"target_col\"]\n",
    "    top_n = query[\"top_n\"]\n",
    "    \n",
    "    print(f\"Top {top_n} sentences for {speaker_id} on target {target_col}:\")\n",
    "    top_sentences = find_top_n_sentences(df, speaker_id, target_col, top_n)\n",
    "    print(top_sentences)\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
